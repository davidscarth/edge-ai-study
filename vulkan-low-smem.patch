--- a/ggml/src/ggml-vulkan/ggml-vulkan.cpp
+++ b/ggml/src/ggml-vulkan/ggml-vulkan.cpp
@@ -2109,6 +2109,44 @@ static void ggml_vk_load_shaders(vk_device& device) {
     uint32_t l_align, m_align, s_align;
     if (device->coopmat2) {
         // spec constants and tile sizes for non-quant matmul/matmul_id
+        if (device->properties.limits.maxComputeSharedMemorySize <= 16384) {
+            // Low SMEM path for devices with <= 16KB of shared memory.
+            // WARP=32 values for better driver compatibility.
+            l_warptile = { 128,  64, 128, 32, 1 };
+            m_warptile = { 128,  64,  64, 32, 0 };
+            s_warptile = {  64,  32,  64, 32, 0 };
+            l_wg_denoms = { 64, 128, 1 };
+            m_wg_denoms = { 64, 64, 1 };
+            s_wg_denoms = { 32, 64, 1 };
+
+            // spec constants and tile sizes for quant matmul (non-Qi_K)
+            l_warptile_mmq = { 128, 64, 128, 32, 1 };
+            m_warptile_mmq = { 128, 64, 64, 32, 1 };
+            s_warptile_mmq = { 128, 32, 32, 64, 0 };
+            l_mmq_wg_denoms = { 64, 128, 1 };
+            m_mmq_wg_denoms = { 64, 64, 1 };
+            s_mmq_wg_denoms = { 32, 32, 1 };
+
+            // spec constants and tile sizes for quant matmul (Qi_K) - same as non-Qi_K for low SMEM
+            l_warptile_mmq_k = l_warptile_mmq;
+            m_warptile_mmq_k = m_warptile_mmq;
+            s_warptile_mmq_k = s_warptile_mmq;
+            l_mmq_wg_denoms_k = l_mmq_wg_denoms;
+            m_mmq_wg_denoms_k = m_mmq_wg_denoms;
+            s_mmq_wg_denoms_k = s_mmq_wg_denoms;
+
+            // spec constants and tile sizes for quant matmul_id
+            l_warptile_mmqid = { 128, 64, 64, 16, 0 };
+            m_warptile_mmqid = { 128, 32, 64, 16, 0 };
+            s_warptile_mmqid = { 128, 32, 32, 16, 0 };
+            l_mmqid_wg_denoms = { 64, 64, 1 };
+            m_mmqid_wg_denoms = { 32, 64, 1 };
+            s_mmqid_wg_denoms = { 32, 32, 1 };
+
+            l_align = 64;
+            m_align = 32;
+            s_align = 32;
+        } else {
         l_warptile = { 256, 128, 256, 64, 1 };
         m_warptile = { 256, 128, 128, 64, 0 };
         s_warptile = { 128,  64,  64, 64, 0 };
@@ -2143,6 +2181,7 @@ static void ggml_vk_load_shaders(vk_device& device) {
         l_align = 128;
         m_align =  64;
         s_align =  32;
+        }
     } else {
         // Matrix cores require different warp group sizes
         const uint32_t tm_l = device->coopmat_support ? device->coopmat_m : 4;
@@ -2155,6 +2194,26 @@ static void ggml_vk_load_shaders(vk_device& device) {
         const uint32_t tk_m = device->coopmat_support ? device->coopmat_k : 1;
         const uint32_t tk_s = device->coopmat_support ? device->coopmat_k : 1;
 
+        if (device->properties.limits.maxComputeSharedMemorySize <= 16384) {
+            // Low SMEM path for devices with <= 16KB of shared memory.
+            l_warptile = { 128, 128, 128, 16, 32 * 2, 64, 2, tm_l, tn_l, tk_l, 32 };
+            m_warptile = { 128,  64,  64, 16, 32, 32, 2, tm_m, tn_m, tk_m, 32 };
+            s_warptile = { 32, 32, 32, 16, 32, 32, 2, tm_s, tn_s, tk_s, 32 };
+
+            for (uint32_t i = 0; i < GGML_TYPE_COUNT; ++i) {
+                ggml_type t = (ggml_type)i;
+                if (!ggml_vk_matmul_shmem_support(device, s_warptile, false, t)) {
+                    device->mul_mat_s[i] = false;
+                    device->mul_mat_m[i] = false;
+                    device->mul_mat_l[i] = false;
+                }
+                if (!ggml_vk_matmul_shmem_support(device, s_warptile_mmqid, true, t)) {
+                    device->mul_mat_id_s[i] = false;
+                    device->mul_mat_id_m[i] = false;
+                    device->mul_mat_id_l[i] = false;
+                }
+            }
+        } else {
         l_warptile = { 128, 128, 128, 16, subgroup_size_8 * 2, 64, 2, tm_l, tn_l, tk_l, subgroup_size_8 };
         m_warptile = { 128,  64,  64, 16, subgroup_size_8, 32, 2, tm_m, tn_m, tk_m, subgroup_size_8 };
         s_warptile = { subgroup_size_16, 32, 32, 16, 32, 32, 2, tm_s, tn_s, tk_s, subgroup_size_8 };
@@ -2207,6 +2266,7 @@ static void ggml_vk_load_shaders(vk_device& device) {
             }
         }
     }
+        }
 
     if (!device->pipeline_matmul_f32) {
         device->pipeline_matmul_f32 = std::make_shared<vk_matmul_pipeline_struct>();
@@ -5476,6 +5536,17 @@ static void ggml_vk_mul_mat_q_f16(ggml_backend_vk_context * ctx, vk_context& sub
 
     vk_pipeline pipeline = ggml_vk_guess_matmul_pipeline(ctx, mmp, ne01, ne11, aligned, qx_needs_dequant ? f16_type : src0->type, quantize_y ? GGML_TYPE_Q8_1 : (y_f32_kernel ? GGML_TYPE_F32 : src1->type));
 
+    if (pipeline == nullptr) {
+        if (dryrun) {
+            // In dry run, we can't fail, just mark as needing compilation and provide dummy data.
+            // The actual pipeline will be created later.
+            ctx->device->need_compiles = true;
+            return;
+        }
+        GGML_ABORT("fatal error: could not find a suitable matmul pipeline.");
+    }
+
+
     // Reserve extra storage in the N dimension for the Y matrix, so we can avoid bounds-checking
     uint32_t padded_n = qy_needs_dequant ? ROUNDUP_POW2(ne11, pipeline->wg_denoms[1]) : ne11;
     const int x_ne = ne01 * ne00;


