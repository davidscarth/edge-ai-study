--- a/ggml/src/ggml-vulkan/ggml-vulkan.cpp
+++ b/ggml/src/ggml-vulkan/ggml-vulkan.cpp
@@ -2109,6 +2109,44 @@
     uint32_t l_align, m_align, s_align;
     if (device->coopmat2) {
         // spec constants and tile sizes for non-quant matmul/matmul_id
+        if (device->properties.limits.maxComputeSharedMemorySize <= 16384) {
+            // Low SMEM path for devices with <= 16KB of shared memory.
+            // WARP=32 values for better driver compatibility.
+            l_warptile = { 128,  64, 128, 32, 1 };
+            m_warptile = { 128,  64,  64, 32, 0 };
+            s_warptile = {  64,  32,  64, 32, 0 };
+            l_wg_denoms = { 64, 128, 1 };
+            m_wg_denoms = { 64, 64, 1 };
+            s_wg_denoms = { 32, 64, 1 };
+
+            // spec constants and tile sizes for quant matmul (non-Qi_K)
+            l_warptile_mmq = { 128, 64, 128, 32, 1 };
+            m_warptile_mmq = { 128, 64, 64, 32, 1 };
+            s_warptile_mmq = { 128, 32, 32, 64, 0 };
+            l_mmq_wg_denoms = { 64, 128, 1 };
+            m_mmq_wg_denoms = { 64, 64, 1 };
+            s_mmq_wg_denoms = { 32, 32, 1 };
+
+            // spec constants and tile sizes for quant matmul (Qi_K) - same as non-Qi_K for low SMEM
+            l_warptile_mmq_k = l_warptile_mmq;
+            m_warptile_mmq_k = m_warptile_mmq;
+            s_warptile_mmq_k = s_warptile_mmq;
+            l_mmq_wg_denoms_k = l_mmq_wg_denoms;
+            m_mmq_wg_denoms_k = m_mmq_wg_denoms;
+            s_mmq_wg_denoms_k = s_mmq_wg_denoms;
+
+            // spec constants and tile sizes for quant matmul_id
+            l_warptile_mmqid = { 128, 64, 64, 16, 0 };
+            m_warptile_mmqid = { 128, 32, 64, 16, 0 };
+            s_warptile_mmqid = { 128, 32, 32, 16, 0 };
+            l_mmqid_wg_denoms = { 64, 64, 1 };
+            m_mmqid_wg_denoms = { 32, 64, 1 };
+            s_mmqid_wg_denoms = { 32, 32, 1 };
+
+            l_align = 64;
+            m_align = 32;
+            s_align = 32;
+        } else {
         l_warptile = { 256, 128, 256, 64, 1 };
         m_warptile = { 256, 128, 128, 64, 0 };
         s_warptile = { 128,  64,  64, 64, 0 };
@@ -2143,6 +2181,7 @@
         l_align = 128;
         m_align =  64;
         s_align =  32;
+        }
     } else {
         // Matrix cores require different warp group sizes
         const uint32_t tm_l = device->coopmat_support ? device->coopmat_m : 4;
@@ -2155,55 +2194,88 @@
         const uint32_t tk_m = device->coopmat_support ? device->coopmat_k : 1;
         const uint32_t tk_s = device->coopmat_support ? device->coopmat_k : 1;
 
-        l_warptile = { 128, 128, 128, 16, subgroup_size_8 * 2, 64, 2, tm_l, tn_l, tk_l, subgroup_size_8 };
-        m_warptile = { 128,  64,  64, 16, subgroup_size_8, 32, 2, tm_m, tn_m, tk_m, subgroup_size_8 };
-        s_warptile = { subgroup_size_16, 32, 32, 16, 32, 32, 2, tm_s, tn_s, tk_s, subgroup_size_8 };
-
-        l_warptile_mmq = { 128, 128, 128, 32, subgroup_size_8 * 2, 64, 2, tm_l, tn_l, tk_l, subgroup_size_8 };
-        m_warptile_mmq = { 128,  64,  64, 32, subgroup_size_8, 32, 2, tm_m, tn_m, tk_m, subgroup_size_8 };
-        s_warptile_mmq = { subgroup_size_32, 32, 32, 32, 32, 32, 2, tm_s, tn_s, tk_s, subgroup_size_8 };
-
-        l_warptile_mmq_int = { 128, 128, 128, 32, subgroup_size_8 * 2, 64, 2, 4, 4, 1, subgroup_size_8 };
-        m_warptile_mmq_int = { 128,  64,  64, 32, subgroup_size_8,     32, 2, 2, 2, 1, subgroup_size_8 };
-        s_warptile_mmq_int = { subgroup_size_32, 32, 32, 32, 32,       32, 2, 2, 1, 1, subgroup_size_8 };
-
-        // chip specific tuning
-        if ((device->architecture == AMD_GCN) && (device->driver_id != vk::DriverId::eAmdProprietary)) {
-            m_warptile_mmq = m_warptile_mmq_int = { 256, 64, 64, 32, 16, 16, 2, 2, 2, 1, 16 };
-        }
-
-        l_mmq_wg_denoms = l_wg_denoms = {128, 128, 1 };
-        m_mmq_wg_denoms = m_wg_denoms = { 64,  64, 1 };
-        s_mmq_wg_denoms = s_wg_denoms = { 32,  32, 1 };
-        l_align = 128;
-        m_align =  64;
-        s_align =  32;
-
-        for (uint32_t i = 0; i < GGML_TYPE_COUNT; ++i) {
-            ggml_type t = (ggml_type)i;
-            // Disable medium and large matrix multiplication if not enough shared memory is available
-            // Check mmq warptiles as the largest configuration
-            // Throw an error if not enough for any matrix multiplication is available
-            if (!ggml_vk_matmul_shmem_support(device, s_warptile_mmq, false, t)) {
-                std::cerr << "ggml_vulkan: Error: Shared memory size too small for matrix multiplication." << std::endl;
-                throw std::runtime_error("Shared memory size too small for matrix multiplication.");
-            } else if (!ggml_vk_matmul_shmem_support(device, m_warptile_mmq, false, t)) {
-                device->mul_mat_m[i] = false;
-                device->mul_mat_l[i] = false;
-            } else if (!ggml_vk_matmul_shmem_support(device, l_warptile_mmq, false, t)) {
-                device->mul_mat_l[i] = false;
+        if (device->properties.limits.maxComputeSharedMemorySize <= 16384) {
+            // Low SMEM path for devices with <= 16KB of shared memory.
+            l_warptile = { 128, 128, 128, 16, 32 * 2, 64, 2, tm_l, tn_l, tk_l, 32 };
+            m_warptile = { 128,  64,  64, 16, 32, 32, 2, tm_m, tn_m, tk_m, 32 };
+            s_warptile = { 32, 32, 32, 16, 32, 32, 2, tm_s, tn_s, tk_s, 32 };
+
+            for (uint32_t i = 0; i < GGML_TYPE_COUNT; ++i) {
+                ggml_type t = (ggml_type)i;
+                if (!ggml_vk_matmul_shmem_support(device, s_warptile, false, t)) {
+                    device->mul_mat_s[i] = false;
+                    device->mul_mat_m[i] = false;
+                    device->mul_mat_l[i] = false;
+                } else if (!ggml_vk_matmul_shmem_support(device, m_warptile, false, t)) {
+                    device->mul_mat_m[i] = false;
+                    device->mul_mat_l[i] = false;
+                } else if (!ggml_vk_matmul_shmem_support(device, l_warptile, false, t)) {
+                    device->mul_mat_l[i] = false;
+                }
+
+                if (!ggml_vk_matmul_shmem_support(device, s_warptile, true, t)) {
+                    device->mul_mat_id_s[i] = false;
+                    device->mul_mat_id_m[i] = false;
+                    device->mul_mat_id_l[i] = false;
+                } else if (!ggml_vk_matmul_shmem_support(device, m_warptile, true, t)) {
+                    device->mul_mat_id_m[i] = false;
+                    device->mul_mat_id_l[i] = false;
+                } else if (!ggml_vk_matmul_shmem_support(device, l_warptile, true, t)) {
+                    device->mul_mat_id_l[i] = false;
+                }
             }
-
-            // Disable mul_mat_id if not enough shared memory is available
-            if (!ggml_vk_matmul_shmem_support(device, s_warptile_mmq, true, t)) {
-                device->mul_mat_id_s[i] = false;
-                device->mul_mat_id_m[i] = false;
-                device->mul_mat_id_l[i] = false;
-            } else if (!ggml_vk_matmul_shmem_support(device, m_warptile_mmq, true, t)) {
-                device->mul_mat_id_m[i] = false;
-                device->mul_mat_id_l[i] = false;
-            } else if (!ggml_vk_matmul_shmem_support(device, l_warptile_mmq, true, t)) {
-                device->mul_mat_id_l[i] = false;
+        } else {
+            // Standard device path (original code)
+            l_warptile = { 128, 128, 128, 16, subgroup_size_8 * 2, 64, 2, tm_l, tn_l, tk_l, subgroup_size_8 };
+            m_warptile = { 128,  64,  64, 16, subgroup_size_8, 32, 2, tm_m, tn_m, tk_m, subgroup_size_8 };
+            s_warptile = { subgroup_size_16, 32, 32, 16, 32, 32, 2, tm_s, tn_s, tk_s, subgroup_size_8 };
+
+            l_warptile_mmq = { 128, 128, 128, 32, subgroup_size_8 * 2, 64, 2, tm_l, tn_l, tk_l, subgroup_size_8 };
+            m_warptile_mmq = { 128,  64,  64, 32, subgroup_size_8, 32, 2, tm_m, tn_m, tk_m, subgroup_size_8 };
+            s_warptile_mmq = { subgroup_size_32, 32, 32, 32, 32, 32, 2, tm_s, tn_s, tk_s, subgroup_size_8 };
+
+            l_warptile_mmq_int = { 128, 128, 128, 32, subgroup_size_8 * 2, 64, 2, 4, 4, 1, subgroup_size_8 };
+            m_warptile_mmq_int = { 128,  64,  64, 32, subgroup_size_8,     32, 2, 2, 2, 1, subgroup_size_8 };
+            s_warptile_mmq_int = { subgroup_size_32, 32, 32, 32, 32,       32, 2, 2, 1, 1, subgroup_size_8 };
+
+            // chip specific tuning
+            if ((device->architecture == AMD_GCN) && (device->driver_id != vk::DriverId::eAmdProprietary)) {
+                m_warptile_mmq = m_warptile_mmq_int = { 256, 64, 64, 32, 16, 16, 2, 2, 2, 1, 16 };
+            }
+
+            l_mmq_wg_denoms = l_wg_denoms = {128, 128, 1 };
+            m_mmq_wg_denoms = m_wg_denoms = { 64,  64, 1 };
+            s_mmq_wg_denoms = s_wg_denoms = { 32,  32, 1 };
+            l_align = 128;
+            m_align =  64;
+            s_align =  32;
+
+            for (uint32_t i = 0; i < GGML_TYPE_COUNT; ++i) {
+                ggml_type t = (ggml_type)i;
+                // Disable medium and large matrix multiplication if not enough shared memory is available
+                // Check mmq warptiles as the largest configuration
+                // Throw an error if not enough for any matrix multiplication is available
+                if (!ggml_vk_matmul_shmem_support(device, s_warptile_mmq, false, t)) {
+                    std::cerr << "ggml_vulkan: Error: Shared memory size too small for matrix multiplication." << std::endl;
+                    throw std::runtime_error("Shared memory size too small for matrix multiplication.");
+                } else if (!ggml_vk_matmul_shmem_support(device, m_warptile_mmq, false, t)) {
+                    device->mul_mat_m[i] = false;
+                    device->mul_mat_l[i] = false;
+                } else if (!ggml_vk_matmul_shmem_support(device, l_warptile_mmq, false, t)) {
+                    device->mul_mat_l[i] = false;
+                }
+
+                // Disable mul_mat_id if not enough shared memory is available
+                if (!ggml_vk_matmul_shmem_support(device, s_warptile_mmq, true, t)) {
+                    device->mul_mat_id_s[i] = false;
+                    device->mul_mat_id_m[i] = false;
+                    device->mul_mat_id_l[i] = false;
+                } else if (!ggml_vk_matmul_shmem_support(device, m_warptile_mmq, true, t)) {
+                    device->mul_mat_id_m[i] = false;
+                    device->mul_mat_id_l[i] = false;
+                } else if (!ggml_vk_matmul_shmem_support(device, l_warptile_mmq, true, t)) {
+                    device->mul_mat_id_l[i] = false;
+                }
             }
         }
     }
@@ -5476,6 +5548,16 @@
 
     vk_pipeline pipeline = ggml_vk_guess_matmul_pipeline(ctx, mmp, ne01, ne11, aligned, qx_needs_dequant ? f16_type : src0->type, quantize_y ? GGML_TYPE_Q8_1 : (y_f32_kernel ? GGML_TYPE_F32 : src1->type));
 
+    if (pipeline == nullptr) {
+        if (dryrun) {
+            // In dry run, we can't fail, just mark as needing compilation and provide dummy data.
+            // The actual pipeline will be created later.
+            ctx->device->need_compiles = true;
+            return;
+        }
+        GGML_ABORT("fatal error: could not find a suitable matmul pipeline.");
+    }
+
     // Reserve extra storage in the N dimension for the Y matrix, so we can avoid bounds-checking
     uint32_t padded_n = qy_needs_dequant ? ROUNDUP_POW2(ne11, pipeline->wg_denoms[1]) : ne11;
     const int x_ne = ne01 * ne00;
@@ -6098,6 +6180,16 @@
     const bool aligned = ne10 == kpad && ne01 > 8 && nei1 > 8;
 
     vk_pipeline pipeline = ggml_vk_guess_matmul_id_pipeline(ctx, mmp, ne01, nei1, aligned, qx_needs_dequant ? f16_type : src0->type);
+
+    if (pipeline == nullptr) {
+        if (dryrun) {
+            // In dry run, we can't fail, just mark as needing compilation and provide dummy data.
+            // The actual pipeline will be created later.
+            ctx->device->need_compiles = true;
+            return;
+        }
+        GGML_ABORT("fatal error: could not find a suitable matmul_id pipeline.");
+    }
 
     // Reserve extra storage in the N dimension for the Y matrix, so we can avoid bounds-checking
     uint32_t padded_n = qy_needs_dequant ? ROUNDUP_POW2(ne11, pipeline->wg_denoms[1]) :ne11;
